{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/home/Learn/Master/3. Multimedia/AIChallenge/Code/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/home/Learn/Master/3. Multimedia/AIChallenge/Code/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/home/Learn/Master/3. Multimedia/AIChallenge/Code/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photography of a city skyline with a large logo in the middle\n",
      "a close up of a city skyline with a large orange sign\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "img_url = 'keyframes/L01_V001/001.jpg' \n",
    "# raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "raw_image = Image.open(img_url).convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the image includes a city skyline and a river with boats\n"
     ]
    }
   ],
   "source": [
    "text = \"In the image includes\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photography of a flooded street with a truck driving through it\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_url = 'keyframes/L01_V001/016.jpg' \n",
    "# raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "raw_image = Image.open(img_url).convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.1:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "You are an translation assistant for language translation tasks. You need to translate english sentence to vietnamese and response only the Vietnamese result.\n",
    "\n",
    "English :{english}\n",
    "Vietnamese: \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_llm(english):\n",
    "  prompt_formatted = prompt.format(english=english)\n",
    "  res = llm.invoke(prompt_formatted)\n",
    "  # print(prompt_formatted)\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hình ảnh đường phố bị ngập nước với một xe tải đang chạy qua.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_llm(\"a photography of a flooded street with a truck driving through it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 44651.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from pymilvus.model.hybrid import BGEM3EmbeddingFunction\n",
    "\n",
    "bge_m3_ef = BGEM3EmbeddingFunction(\n",
    "    model_name='BAAI/bge-m3', # Specify the model name\n",
    "    device='cpu', # Specify the device to use, e.g., 'cpu' or 'cuda:0'\n",
    "    use_fp16=False # Specify whether to use fp16. Set to `False` if `device` is `cpu`.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= [f\"Hình ảnh đường phố bị ngập nước với một xe tải đang chạy qua.\"]\n",
    "docs_embeddings = bge_m3_ef.encode_documents(docs)\n",
    "\n",
    "vectors = docs_embeddings[\"dense\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.04253798,  0.00780115, -0.03375866, ...,  0.0280426 ,\n",
       "        -0.04685987, -0.02310734], dtype=float32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "client = MilvusClient(\n",
    "    uri=\"http://localhost:19530\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.has_collection(collection_name=\"ai_challenge_collection\"):\n",
    "    client.drop_collection(collection_name=\"ai_challenge_collection\")\n",
    "client.create_collection(\n",
    "        collection_name=\"ai_challenge_collection\",\n",
    "        dimension=1024,  # The vectors we will use in this demo has 768 dimensions\n",
    "        auto_id=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'insert_count': 1, 'ids': [452540126240705757]}\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "data.append({ \"vector\": vectors[0], \"text\": docs[0]} )\n",
    "res = client.insert(collection_name=\"ai_challenge_collection\", data=data)        \n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 452540126240705757, 'text': 'Hình ảnh đường phố bị ngập nước với một xe tải đang chạy qua.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res = client.query(\n",
    "    collection_name=\"ai_challenge_collection\",  # target collection\n",
    "    filter='',  # number of returned entities\n",
    "    limit=100,\n",
    "    output_fields=[\"id\", 'text'],  # specifies fields to be returned\n",
    "    # sorted=True, #\n",
    ")\n",
    "for item in res:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 452540126240705757, 'distance': 0.8477668762207031, 'entity': {'text': 'Hình ảnh đường phố bị ngập nước với một xe tải đang chạy qua.', 'id': 452540126240705757}}]\n"
     ]
    }
   ],
   "source": [
    "queries = [\"Tìm ảnh thành phố ngập nước có một xe tải đang chạy\"]\n",
    "\n",
    "query_embeddings = bge_m3_ef.encode_queries(queries)\n",
    "\n",
    "res = client.search(\n",
    "    collection_name=\"ai_challenge_collection\",  # target collection\n",
    "    data=[query_embeddings['dense'][0]],  # query vectors\n",
    "    limit=5,  # number of returned entities\n",
    "    output_fields=[\"id\",\"text\"],  # specifies fields to be returned\n",
    ")\n",
    "context_items = res[0]\n",
    "\n",
    "print(context_items)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
